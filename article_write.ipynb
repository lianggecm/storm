{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dspy in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (2.5.27)\n",
      "Requirement already satisfied: backoff in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (2.2.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (2.15.0)\n",
      "Requirement already satisfied: diskcache in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (5.6.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (0.24.1)\n",
      "Requirement already satisfied: joblib~=1.3 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (1.4.2)\n",
      "Requirement already satisfied: json-repair in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (0.30.1)\n",
      "Requirement already satisfied: litellm==1.51.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (1.51.0)\n",
      "Requirement already satisfied: magicattr~=0.1.6 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (0.1.6)\n",
      "Requirement already satisfied: openai in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (1.54.2)\n",
      "Requirement already satisfied: optuna in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (4.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (2.1.4)\n",
      "Requirement already satisfied: pydantic~=2.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (2.8.2)\n",
      "Requirement already satisfied: regex in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (2.31.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (8.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (4.66.2)\n",
      "Requirement already satisfied: ujson in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from dspy) (5.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from litellm==1.51.0->dspy) (3.9.3)\n",
      "Requirement already satisfied: click in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from litellm==1.51.0->dspy) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from litellm==1.51.0->dspy) (6.8.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from litellm==1.51.0->dspy) (3.1.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from litellm==1.51.0->dspy) (4.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from litellm==1.51.0->dspy) (1.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from litellm==1.51.0->dspy) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from litellm==1.51.0->dspy) (0.20.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from openai->dspy) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from openai->dspy) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from openai->dspy) (0.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from openai->dspy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from openai->dspy) (4.12.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from httpx->dspy) (2023.11.17)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from httpx->dspy) (0.17.3)\n",
      "Requirement already satisfied: idna in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from httpx->dspy) (3.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic~=2.0->dspy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic~=2.0->dspy) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->dspy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->dspy) (2.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from tqdm->dspy) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets->dspy) (1.26.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets->dspy) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets->dspy) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets->dspy) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets->dspy) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets->dspy) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->dspy) (2023.9.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets->dspy) (0.26.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets->dspy) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets->dspy) (6.0.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from optuna->dspy) (1.14.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from optuna->dspy) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from optuna->dspy) (2.0.21)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from pandas->dspy) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from pandas->dspy) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from pandas->dspy) (2023.3)\n",
      "Requirement already satisfied: Mako in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from alembic>=1.5.0->optuna->dspy) (1.3.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->litellm==1.51.0->dspy) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->litellm==1.51.0->dspy) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->litellm==1.51.0->dspy) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->litellm==1.51.0->dspy) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->litellm==1.51.0->dspy) (1.8.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->dspy) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets->dspy) (3.15.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm==1.51.0->dspy) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.51.0->dspy) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.51.0->dspy) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.51.0->dspy) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.51.0->dspy) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->dspy) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\liang\\anaconda3\\envs\\ml\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna->dspy) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs\n",
    "from knowledge_storm.lm import OpenAIModel, GoogleModel\n",
    "from knowledge_storm.rm import DuckDuckGoSearchRM, TavilySearchRM\n",
    "\n",
    "\n",
    "lm_configs = STORMWikiLMConfigs()\n",
    "openai_kwargs = {\n",
    "    'api_key': os.getenv(\"OPENAI_API_KEY\"),\n",
    "    'temperature': 1.0,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.\n",
    "# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.\n",
    "# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.\n",
    "if True:\n",
    "    gpt_35 = OpenAIModel(model='gpt-3.5-turbo', max_tokens=500, **openai_kwargs)\n",
    "    gpt_4 = OpenAIModel(model='gpt-4o-mini', max_tokens=4500, **openai_kwargs)\n",
    "    lm_configs.set_conv_simulator_lm(gpt_35)\n",
    "    lm_configs.set_question_asker_lm(gpt_35)\n",
    "    lm_configs.set_outline_gen_lm(gpt_4)\n",
    "    lm_configs.set_article_gen_lm(gpt_4)\n",
    "    lm_configs.set_article_polish_lm(gpt_4)\n",
    "\n",
    "\n",
    "if False:\n",
    "    # Use gemini\n",
    "    gemini_kwargs = {\n",
    "        'api_key': os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        'temperature': 1.0,\n",
    "        'top_p': 0.9,\n",
    "        'max_tokens': 800,\n",
    "    }\n",
    "    gemini_kwargs1 = {\n",
    "        'api_key': os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        'temperature': 1.0,\n",
    "        'top_p': 0.9,\n",
    "        'max_tokens': 4000,\n",
    "    }\n",
    "    conv_simulator_lm = GoogleModel(model='models/gemini-1.5-flash', **gemini_kwargs)\n",
    "    question_asker_lm = GoogleModel(model='models/gemini-1.5-flash',  **gemini_kwargs)\n",
    "    outline_gen_lm = GoogleModel(model='models/gemini-1.5-pro-exp-0801',  **gemini_kwargs)\n",
    "    article_gen_lm = GoogleModel(model='models/gemini-1.5-pro-exp-0801',  **gemini_kwargs)\n",
    "    article_polish_lm = GoogleModel(model='models/gemini-1.5-pro-exp-0801',  **gemini_kwargs1)\n",
    "\n",
    "    lm_configs.set_conv_simulator_lm(conv_simulator_lm)\n",
    "    lm_configs.set_question_asker_lm(question_asker_lm)\n",
    "    lm_configs.set_outline_gen_lm(outline_gen_lm)\n",
    "    lm_configs.set_article_gen_lm(article_gen_lm)\n",
    "    lm_configs.set_article_polish_lm(article_polish_lm)\n",
    "# Check out the STORMWikiRunnerArguments class for more configurations.\n",
    "engine_args = STORMWikiRunnerArguments( output_dir=\"./articles\",\n",
    "                                       max_conv_turn=3,\n",
    "                                       max_perspective=10,\n",
    "                                       search_top_k=10,\n",
    "                                       \n",
    ")\n",
    "rm = TavilySearchRM(tavily_search_api_key=os.getenv('TAVILY_API_KEY'),k=engine_args.search_top_k)\n",
    "runner = STORMWikiRunner(engine_args, lm_configs, rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=\"\"\"假设你是个资深的半导体晶圆代工行业的从业者。也是Data+AI的专家。目前正在规划一个新项目，目标是建立一个AI驱动的database system，包括内容如下：\n",
    "\n",
    "\"范畴如下: 晶圆代工厂的RD（研发）数据库以R&D所有数据为范畴，重点为28 nm到20 nm技术节点，包含客户设计档案，文件，R&D模拟档案，R&D设备以及量测数据，文件，以及R&D输出文件及档案。\n",
    "\n",
    "长远目标：在R&D部门实现AI辅助的数据分析能力，以提高效率，降低成本，提高质量。\n",
    "\n",
    "\n",
    "\n",
    "目前规划中的一些AI赋能的方向包括：\n",
    "\n",
    "1， 设计端: DRC, layout optimization, lithography simulation, 明年会先做一个系统来串联以上三子系统，建立思维链，验证链，以及价值链\n",
    "\n",
    "2， 制程端包括有 APC, FDC, YPA, WRA, PVA\n",
    "\n",
    "\n",
    "\n",
    "AI引入分两步走，明年的目标集中在通过AI进行预测为重点，后年集中在通过AI来优化生产。全系统整合会以STCO为主。\"\n",
    "\n",
    "请列出详细步骤如何来建立这么一个数据库系统， 对于具体的实现步骤，请提供详细的资料，包括数据源，数据具体流程，如何通过AI辅助数据分析等等。请一步步展开具体的实施计划。就如何具体利用LLM技术以及数据的结合，展开阐述方案。\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentence_transformers.SentenceTransformer : INFO     : Use pytorch device_name: cpu\n",
      "sentence_transformers.SentenceTransformer : INFO     : Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "knowledge_storm.interface : INFO     : run_article_generation_module executed in 17.9130 seconds\n",
      "knowledge_storm.interface : INFO     : run_article_polishing_module executed in 0.0179 seconds\n"
     ]
    }
   ],
   "source": [
    "runner.run(\n",
    "        topic=topic,\n",
    "        short_topic=\"NewArticle_V2\",\n",
    "        do_research=False,\n",
    "        do_generate_outline=False,\n",
    "        do_generate_article=True,\n",
    "        do_polish_article=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Execution time *****\n",
      "run_knowledge_curation_module: 52.7618 seconds\n",
      "run_outline_generation_module: 6.1668 seconds\n",
      "run_article_generation_module: 17.9130 seconds\n",
      "run_article_polishing_module: 0.0179 seconds\n",
      "***** Token usage of language models: *****\n",
      "run_knowledge_curation_module\n",
      "    gpt-3.5-turbo: {'prompt_tokens': 47960, 'completion_tokens': 7777}\n",
      "    gpt-4o-mini: {'prompt_tokens': 0, 'completion_tokens': 0}\n",
      "run_outline_generation_module\n",
      "    gpt-3.5-turbo: {'prompt_tokens': 0, 'completion_tokens': 0}\n",
      "    gpt-4o-mini: {'prompt_tokens': 6965, 'completion_tokens': 860}\n",
      "run_article_generation_module\n",
      "    gpt-3.5-turbo: {'prompt_tokens': 0, 'completion_tokens': 0}\n",
      "    gpt-4o-mini: {'prompt_tokens': 13123, 'completion_tokens': 4501}\n",
      "run_article_polishing_module\n",
      "    gpt-3.5-turbo: {'prompt_tokens': 0, 'completion_tokens': 0}\n",
      "    gpt-4o-mini: {'prompt_tokens': 4696, 'completion_tokens': 375}\n",
      "***** Number of queries of retrieval models: *****\n",
      "run_knowledge_curation_module: {'TavilySearchRM': 36}\n",
      "run_outline_generation_module: {'TavilySearchRM': 0}\n",
      "run_article_generation_module: {'TavilySearchRM': 0}\n",
      "run_article_polishing_module: {'TavilySearchRM': 0}\n"
     ]
    }
   ],
   "source": [
    "runner.post_run()\n",
    "runner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_bibliography_from_url_to_info(url_to_info):\n",
    "    bibliography_list = []\n",
    "    sorted_url_to_unified_index = dict(sorted(url_to_info['url_to_unified_index'].items(),\n",
    "                                                key=lambda item: item[1]))\n",
    "    for url, index in sorted_url_to_unified_index.items():\n",
    "        title = url_to_info['url_to_info'][url]['title']\n",
    "        bibliography_list.append(f\"[{index}]: [{title}]({url})\")\n",
    "    bibliography_string = \"\\n\\n\".join(bibliography_list)\n",
    "    return f\"# References\\n\\n{bibliography_string}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('articles/NewArticle_V2/url_to_info.json') as f:\n",
    "    url_list=json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# References\\n\\n[1]: [A step-by-step guide to R&D data management - BioLizard](https://lizard.bio/knowledge-hub/guide-rd-data-management)\\n\\n[2]: [FAIR principles for AI models with a practical application for ...](https://www.nature.com/articles/s41597-022-01712-9)\\n\\n[3]: [The Complexities and Challenges of Integrating LLMs into Applications](https://towardsdatascience.com/the-complexities-and-challenges-of-integrating-llm-into-applications-913d4461bbe0)\\n\\n[4]: [A review of artificial intelligence applications in manufacturing ...](https://aiche.onlinelibrary.wiley.com/doi/full/10.1002/amp2.10159)\\n\\n[5]: [Using the power of advanced analytics to improve manufacturing, R&D ...](https://www.mckinsey.com/~/media/McKinsey/Industries/Semiconductors/Our+Insights/McKinsey+on+Semiconductors+Issue+5+-+Winter+2015/Using+the+power+of+advanced+analytics.ashx)\\n\\n[6]: [PDF](https://www.tcs.com/content/dam/global-tcs/en/pdfs/insights/whitepapers/transforming-semiconductor-manufacturers-supply-chain-advanced-analytics.pdf)\\n\\n[7]: [PDF](https://www.whitehouse.gov/wp-content/uploads/2022/09/PCAST_Semiconductors-Report_Sep2022.pdf)\\n\\n[8]: [Mastering Database Design: An Ultimate Guide - GeeksforGeeks](https://www.geeksforgeeks.org/database-design-ultimate-guide/)\\n\\n[9]: [DB-GPT: Large Language Model Meets Database - Springer](https://link.springer.com/article/10.1007/s41019-023-00235-6)\\n\\n[10]: [Navigating the Future of R&D: Integrating LLMs into Patent ... - XLSCOUT](https://xlscout.ai/navigating-the-future-of-rd-integrating-llms-into-patent-processes/)\\n\\n[11]: [How LLM Applications are Revolutionizing the Manufacturing Industry - Vanti](https://www.vanti.ai/how-llm-applications-are-revolutionizing-the-manufacturing-industry/)\\n\\n[12]: [Smartic: A smart tool for Big Data analytics and IoT](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10806368/)\\n\\n[13]: [Database Design: Principles and Best Practices - GUVI Blogs](https://www.guvi.io/blog/database-design-principles-and-best-practices/)\\n\\n[14]: [9 Best practices for the database design of any system.](https://medium.com/towards-data-engineering/9-best-practices-for-the-database-design-of-any-system-403e1bf1391b)\\n\\n[15]: [Navigating the Integration Maze: Challenges in Incorporating LLM's into ...](https://prolifics.com/us/resource-center/blog/navigating-the-integration-maze-challenges-in-incorporating-llms-into-your-application)\\n\\n[16]: [How to develop and implement a data storage plan - TechTarget](https://www.techtarget.com/searchstorage/tip/How-to-develop-and-implement-a-data-storage-plan)\\n\\n[17]: [PDF](https://assets.kpmg.com/content/dam/kpmg/us/pdf/2017/12/semiconductor-da-brochure.pdf)\\n\\n[18]: [Smartic: A smart tool for Big Data analytics and IoT](https://www.researchgate.net/publication/357663570_Smartic_A_smart_tool_for_Big_Data_analytics_and_IoT)\\n\\n[19]: [Improving the semiconductor industry through advanced analytics](https://www.mckinsey.com/industries/semiconductors/our-insights/improving-the-semiconductor-industry-through-advanced-analytics)\\n\\n[20]: [Artificial Intelligence in Boosting Semiconductor R&D](https://www.patsnap.com/resources/blog/the-role-of-artificial-intelligence-in-accelerating-semiconductor-research-and-development/)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_bibliography_from_url_to_info(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Execution time *****\n",
      "run_knowledge_curation_module: 51.5390 seconds\n",
      "run_outline_generation_module: 13.0449 seconds\n",
      "run_article_generation_module: 36.7257 seconds\n",
      "run_article_polishing_module: 5.8823 seconds\n",
      "***** Token usage of language models: *****\n",
      "run_knowledge_curation_module\n",
      "    gpt-3.5-turbo: {'prompt_tokens': 47990, 'completion_tokens': 8078}\n",
      "    gpt-4o-mini: {'prompt_tokens': 0, 'completion_tokens': 0}\n",
      "run_outline_generation_module\n",
      "    gpt-3.5-turbo: {'prompt_tokens': 0, 'completion_tokens': 0}\n",
      "    gpt-4o-mini: {'prompt_tokens': 6439, 'completion_tokens': 845}\n",
      "run_article_generation_module\n",
      "    gpt-3.5-turbo: {'prompt_tokens': 0, 'completion_tokens': 0}\n",
      "    gpt-4o-mini: {'prompt_tokens': 12347, 'completion_tokens': 4425}\n",
      "run_article_polishing_module\n",
      "    gpt-3.5-turbo: {'prompt_tokens': 0, 'completion_tokens': 0}\n",
      "    gpt-4o-mini: {'prompt_tokens': 4528, 'completion_tokens': 402}\n",
      "***** Number of queries of retrieval models: *****\n",
      "run_knowledge_curation_module: {'TavilySearchRM': 36}\n",
      "run_outline_generation_module: {'TavilySearchRM': 0}\n",
      "run_article_generation_module: {'TavilySearchRM': 0}\n",
      "run_article_polishing_module: {'TavilySearchRM': 0}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_unicode_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Convert Unicode escape sequences to Chinese characters in a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input JSONL file\n",
    "        output_file (str): Path to output JSONL file\n",
    "    \"\"\"\n",
    "    # Convert string paths to Path objects to handle Windows paths correctly\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "    \n",
    "    with codecs.open(input_path, 'r', 'utf-8') as infile, \\\n",
    "         codecs.open(output_path, 'w', 'utf-8') as outfile:\n",
    "        \n",
    "        for line_num, line in enumerate(infile, 1):\n",
    "            try:\n",
    "                # Parse the JSON line\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # Function to recursively process all strings in the JSON object\n",
    "                def convert_strings(obj):\n",
    "                    if isinstance(obj, str):\n",
    "                        try:\n",
    "                            # Try to handle it as a Unicode escape sequence first\n",
    "                            return obj.encode('latin1').decode('unicode-escape').encode('latin1').decode('utf-8')\n",
    "                        except UnicodeError:\n",
    "                            # If that fails, return the original string\n",
    "                            return obj\n",
    "                    elif isinstance(obj, dict):\n",
    "                        return {key: convert_strings(value) for key, value in obj.items()}\n",
    "                    elif isinstance(obj, list):\n",
    "                        return [convert_strings(item) for item in obj]  # Fixed: using obj instead of list\n",
    "                    else:\n",
    "                        return obj\n",
    "                \n",
    "                # Convert all strings in the JSON object\n",
    "                converted_data = convert_strings(data)\n",
    "                \n",
    "                # Write the converted data back as a JSON line\n",
    "                json.dump(converted_data, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON on line {line_num}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {line_num}: {e}\")\n",
    "                print(f\"Problematic line content: {line}\")  # Added debug information\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"articles\\NewArticle_V2\\llm_call_history.jsonl\"\n",
    "    output_file = \"output.jsonl\"\n",
    "    convert_unicode_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RD Database\n",
    "\n",
    "The RD database is a critical component of the semiconductor manufacturing process, specifically focused on research and development (R&D) data spanning technology nodes from 28 nm to 200 nm. This database encompasses a wide range of data types essential for driving AI projects and improving operational efficiency.\n",
    "\n",
    "## Data Scope and Types\n",
    "\n",
    "The RD database will integrate various data sources, including but not limited to:\n",
    "- **Customer Design Files**: These include specifications and layout designs required for chip manufacturing.\n",
    "- **Simulation Archives**: Records of R&D simulations such as Design Rule Checking (DRC), layout optimization, and lithography simulation. The aim is to create a cohesive system that connects these subsystems, thereby establishing a verification and value chain[1].\n",
    "- **Measurement Data**: This encompasses data from various stages of the manufacturing process, including Advanced Process Control (APC), Fault Detection and Classification (FDC), Yield Performance Analysis (YPA), Wafer Reliability Assessment (WRA), and Process Variability Analysis (PVA)[1][2].\n",
    "- **Output Files**: These consist of all final reports, performance metrics, and other documentation arising from the R&D efforts.\n",
    "\n",
    "## Implementation Pathway\n",
    "\n",
    "The implementation pathway for an AI-based data system in the semiconductor wafer foundry industry, particularly focusing on Research and Development (RD) from 28nm to 200nm technology nodes, consists of several structured steps. This pathway is designed to effectively utilize Large Language Models (LLMs) and relevant data sources to create a robust training dataset for the RD AI project.\n",
    "\n",
    "## Future Goals\n",
    "\n",
    "The ultimate objective is to create a robust RD AI project that efficiently trains AI models with high-quality datasets, ultimately enhancing productivity and innovation in semiconductor manufacturing. By utilizing advanced technologies such as LLMs in conjunction with comprehensive data sources, manufacturers can significantly improve their operational capabilities and decision-making processes in the semiconductor domain[3][4].\n",
    "\n",
    "# Technical Nodes\n",
    "\n",
    "## Overview of Technical Nodes\n",
    "\n",
    "In semiconductor manufacturing, technical nodes refer to the specific features and dimensions of the integrated circuits produced, measured in nanometers (nm). As technology progresses, these nodes become increasingly smaller, enabling more transistors to be packed into a given area, thereby enhancing performance and efficiency. Currently, the focus of the project spans technical nodes from 28 nm to 200 nm, reflecting a critical range for both legacy and modern applications in the semiconductor industry[2][5].\n",
    "\n",
    "## Importance of 28 nm to 200 nm Nodes\n",
    "\n",
    "The nodes between 28 nm and 200 nm are particularly significant due to their widespread application in various electronic devices. For instance, the 28 nm node is often associated with high-performance computing and mobile devices, while nodes such as 65 nm and 90 nm are pivotal for consumer electronics, including smartphones and tablets[6][7]. This project aims to establish a comprehensive RD database encompassing design files, measurement data, and production outputs relevant to these technical nodes, ensuring that all critical aspects of semiconductor design and manufacturing are captured and optimized[8][9].\n",
    "\n",
    "## AI-Driven Development\n",
    "\n",
    "The integration of artificial intelligence (AI) into the RD processes at these technical nodes will occur in two phases. The first phase focuses on predictive analytics, utilizing AI to anticipate potential challenges and optimize design parameters based on historical data. For example, reinforcement learning techniques will be employed to enhance optical proximity correction (OPC) recipes, which are vital for ensuring that designs accurately translate to physical products[9][1][5].\n",
    "In the second phase, the emphasis will shift towards using AI for optimization of manufacturing processes. Advanced Process Control (APC) systems will be implemented to monitor real-time production data, enabling dynamic adjustments that enhance yield rates and minimize defects across various technical nodes[5][7]. This approach aligns with the broader Industry 4.0 vision, aiming for fully automated and highly efficient semiconductor manufacturing systems.\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "1. **Data Collection and Integration**: Establish a robust data infrastructure that aggregates customer design files, measurement data, and production outputs from various stages of the RD process for the specified nodes. This involves both structured and unstructured data, such as DRC reports and lithography simulation outputs[8][6].\n",
    "2. **Database Architecture**: Develop a scalable database architecture capable of handling vast amounts of data generated throughout the RD process. This system should support efficient querying and retrieval of information essential for both design and manufacturing optimization[10].\n",
    "3. **AI Model Development**: Utilize large language models (LLMs) to facilitate communication among design, simulation, and manufacturing teams. This step involves training LLMs on relevant technical documents to enhance their understanding and response capabilities in semiconductor design and manufacturing contexts[1][11].\n",
    "4. **Predictive Analytics Implementation**: Deploy AI algorithms focused on predictive analytics to analyze historical data from the RD process. This will assist in identifying patterns and predicting potential outcomes, allowing engineers to make informed decisions early in the design cycle[7].\n",
    "5. **Optimization and Control Systems**: Implement APC systems powered by AI to monitor manufacturing processes in real-time. These systems will enable dynamic adjustments based on sensor data, thereby improving yield rates and operational efficiency across the various technical nodes[5].\n",
    "6. **Feedback Loop and Continuous Improvement**: Establish a feedback mechanism to continually assess and refine the AI models and control systems based on performance metrics and production outcomes. This will ensure that the RD AI project remains aligned with the latest industry advancements and operational needs[11][7].\n",
    "By following these steps, the project aims to create a comprehensive AI-driven data system that supports the efficient and effective development of semiconductor technologies within the critical range of 28 nm to 200 nm nodes.\n",
    "\n",
    "# RD AI Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "The RD AI Project aims to develop a comprehensive AI-driven database that encompasses all relevant data from research and development (R&D) processes within the semiconductor foundry sector, specifically focusing on technology nodes ranging from 28 nm to 200 nm. The project intends to integrate various data types, including client design files, simulation records, measurement data, and output documentation, to establish a robust framework for AI training datasets.\n",
    "\n",
    "## Goals\n",
    "\n",
    "The primary objectives of the RD AI Project are to create a training dataset for AI models that will facilitate enhanced design and manufacturing processes. This initiative is expected to enable more effective decision-making through predictive analytics and optimization techniques applied within semiconductor fabrication and design[12][13].\n",
    "\n",
    "## Implementation Pathway\n",
    "\n",
    "The implementation of the RD AI Project will follow a structured two-phase approach:\n",
    "\n",
    "# Steps to Build an AI-Based Data System\n",
    "\n",
    "## Overview\n",
    "\n",
    "Building an AI-based data system in the semiconductor industry, particularly for the 28 nm to 200 nm technology nodes, requires a structured approach that integrates various data sources and employs advanced AI models. This process is crucial for optimizing the research and development (R&D) functions and facilitating efficient decision-making.\n",
    "\n",
    "## Step 1: Identify Key Objectives and Data Requirements\n",
    "\n",
    "The first step is to clearly define the objectives of the AI project. In this case, the goal is to output an R&D AI project's training dataset that encompasses all relevant data, including customer design files, simulation archives, equipment data, measurement data, and output files.\n",
    "- **Customer Design Files:** Layout designs, DRC (Design Rule Check) reports.\n",
    "- **Simulation Archives:** Lithography simulations and optimization results.\n",
    "- **Process Data:** APC (Advanced Process Control), FDC (Fault Detection and Classification), and other relevant data streams from manufacturing processes.\n",
    "- **Measurement Data:** Metrics related to production quality and process performance.\n",
    "\n",
    "## Step 2: Data Collection and Preprocessing\n",
    "\n",
    "Once the data types are identified, the next step is to collect and preprocess the data.\n",
    "- **Data Collection:** Gathering data from various sources, including legacy systems and current production systems. Challenges may arise due to legacy data management and system complexities, requiring careful planning to avoid data corruption or loss[14][15].\n",
    "- **Data Preprocessing:** Cleaning and structuring the data to ensure it is suitable for AI model training. This may involve standardizing formats, labeling datasets, and removing inconsistencies. The use of observability tools can aid in tracking and managing data throughout this process[16].\n",
    "\n",
    "## Step 3: Implement Large Language Models (LLMs)\n",
    "\n",
    "Utilizing LLMs can significantly enhance the data system.\n",
    "- **Integration with Existing Systems:** Establishing a framework for LLM applications that allows for the consolidation and processing of data while providing a conversational interface for users[4]. This framework should facilitate seamless interaction between human operators and AI systems.\n",
    "- **Training the LLM:** Using the preprocessed datasets to train the LLM, ensuring it understands the context and nuances specific to the semiconductor industry. This could involve fine-tuning the model with domain-specific data to improve its performance on tasks related to design, simulation, and process optimization[17].\n",
    "\n",
    "## Step 4: Develop AI-Powered Analytical Tools\n",
    "\n",
    "Building analytical tools that leverage the LLM's capabilities is crucial for deriving actionable insights.\n",
    "- **Predictive Analytics:** Employing AI techniques to analyze data trends and predict outcomes related to production processes. For example, predicting equipment failures or quality issues before they occur[18][19].\n",
    "- **Optimization Models:** Creating models that use AI to optimize processes based on real-time data, including the identification of process bottlenecks and inefficiencies[20][21].\n",
    "\n",
    "## Step 5: Validate and Iterate\n",
    "\n",
    "After implementing the AI-based data system, it is essential to validate its performance:\n",
    "- **Testing and Validation:** Conducting thorough testing to ensure the system's accuracy and reliability. This may involve running simulations and comparing the AI predictions with actual outcomes.\n",
    "- **Iterative Improvement:** Continuously refining the models based on feedback and new data. This step is crucial as AI models can degrade over time if not regularly updated and maintained[22].\n",
    "\n",
    "# Challenges and Considerations\n",
    "\n",
    "The implementation of AI-based data systems in the semiconductor wafer fabrication industry, particularly for research and development (RD) databases, presents numerous challenges and considerations. Understanding these challenges is critical for successfully integrating AI technologies into semiconductor processes.\n",
    "\n",
    "## Data and System Complexities\n",
    "\n",
    "One of the foremost challenges is managing the complexities inherent in data and system integration. The semiconductor industry generates vast amounts of data, particularly from various technology nodes ranging from 28 nm to 200 nm. This data often resides in disparate legacy systems, which can lead to inefficiencies and difficulties in data retrieval and management[23][11]. Legacy data technology modernization is essential; however, it requires a strategic approach to navigate the complexities of obsolete systems while ensuring data quality for real-time analytics[24][11].\n",
    "\n",
    "## Talent Shortage and Skill Gaps\n",
    "\n",
    "The semiconductor industry is currently facing a significant talent shortage, exacerbated by an aging workforce and the rapid evolution of AI technologies[25]. Companies need skilled professionals such as data scientists, machine learning engineers, and infrastructure architects to develop and implement AI models effectively[26][27]. As AI becomes more integral to RD processes—such as DRC (Design Rule Check), layout optimization, and lithography simulation—there is a pressing need for organizations to recruit or train personnel who possess these specialized skills[28][26].\n",
    "\n",
    "## Over-reliance on Black-box Models\n",
    "\n",
    "Another challenge is the over-reliance on black-box AI models, which can hinder interpretability and transparency in decision-making processes. In the semiconductor manufacturing domain, where precision and reliability are paramount, stakeholders must ensure that AI systems provide actionable insights and are interpretable by engineers and management alike[23][29].\n",
    "\n",
    "## Security and Data Privacy\n",
    "\n",
    "Security and data privacy also present significant hurdles, particularly when dealing with sensitive customer information within AI systems. Many organizations require on-premises deployments to safeguard data, which complicates infrastructure management and scaling efforts[30]. Establishing robust security protocols is essential to mitigate risks while maintaining compliance with regulations regarding data ownership and privacy[31].\n",
    "\n",
    "## Strategic Implementation Paths\n",
    "\n",
    "To navigate these challenges effectively, semiconductor companies should adopt a phased approach to AI development. The initial focus could be on predictive analytics to forecast RD outputs and optimize processes, followed by efforts to enhance production efficiency through AI-driven optimization techniques[25]. A well-defined strategy for data sources and types is critical, including establishing a comprehensive data pipeline that integrates various datasets from RD, such as customer design archives and measurement data, into a unified AI framework[32][33]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RD Database\n",
    "\n",
    "The RD database is a critical component of the semiconductor manufacturing process, specifically focused on research and development (R&D) data spanning technology nodes from 28 nm to 200 nm. This database encompasses a wide range of data types essential for driving AI projects and improving operational efficiency.\n",
    "\n",
    "## Data Scope and Types\n",
    "\n",
    "The RD database will integrate various data sources, including but not limited to:\n",
    "- **Customer Design Files**: These include specifications and layout designs required for chip manufacturing.\n",
    "- **Simulation Archives**: Records of R&D simulations such as Design Rule Checking (DRC), layout optimization, and lithography simulation. The aim is to create a cohesive system that connects these subsystems, thereby establishing a verification and value chain[1].\n",
    "- **Measurement Data**: This encompasses data from various stages of the manufacturing process, including Advanced Process Control (APC), Fault Detection and Classification (FDC), Yield Performance Analysis (YPA), Wafer Reliability Assessment (WRA), and Process Variability Analysis (PVA)[1][2].\n",
    "- **Output Files**: These consist of all final reports, performance metrics, and other documentation arising from the R&D efforts.\n",
    "\n",
    "## Implementation Pathway\n",
    "\n",
    "The implementation pathway for an AI-based data system in the semiconductor wafer foundry industry, particularly focusing on Research and Development (RD) from 28nm to 200nm technology nodes, consists of several structured steps. This pathway is designed to effectively utilize Large Language Models (LLMs) and relevant data sources to create a robust training dataset for the RD AI project.\n",
    "\n",
    "## Future Goals\n",
    "\n",
    "The ultimate objective is to create a robust RD AI project that efficiently trains AI models with high-quality datasets, ultimately enhancing productivity and innovation in semiconductor manufacturing. By utilizing advanced technologies such as LLMs in conjunction with comprehensive data sources, manufacturers can significantly improve their operational capabilities and decision-making processes in the semiconductor domain[3][4].\n",
    "\n",
    "# Technical Nodes\n",
    "\n",
    "## Overview of Technical Nodes\n",
    "\n",
    "In semiconductor manufacturing, technical nodes refer to the specific features and dimensions of the integrated circuits produced, measured in nanometers (nm). As technology progresses, these nodes become increasingly smaller, enabling more transistors to be packed into a given area, thereby enhancing performance and efficiency. Currently, the focus of the project spans technical nodes from 28 nm to 200 nm, reflecting a critical range for both legacy and modern applications in the semiconductor industry[2][5].\n",
    "\n",
    "## Importance of 28 nm to 200 nm Nodes\n",
    "\n",
    "The nodes between 28 nm and 200 nm are particularly significant due to their widespread application in various electronic devices. For instance, the 28 nm node is often associated with high-performance computing and mobile devices, while nodes such as 65 nm and 90 nm are pivotal for consumer electronics, including smartphones and tablets[6][7]. This project aims to establish a comprehensive RD database encompassing design files, measurement data, and production outputs relevant to these technical nodes, ensuring that all critical aspects of semiconductor design and manufacturing are captured and optimized[8][9].\n",
    "\n",
    "## AI-Driven Development\n",
    "\n",
    "The integration of artificial intelligence (AI) into the RD processes at these technical nodes will occur in two phases. The first phase focuses on predictive analytics, utilizing AI to anticipate potential challenges and optimize design parameters based on historical data. For example, reinforcement learning techniques will be employed to enhance optical proximity correction (OPC) recipes, which are vital for ensuring that designs accurately translate to physical products[9][1][5].\n",
    "In the second phase, the emphasis will shift towards using AI for optimization of manufacturing processes. Advanced Process Control (APC) systems will be implemented to monitor real-time production data, enabling dynamic adjustments that enhance yield rates and minimize defects across various technical nodes[5][7]. This approach aligns with the broader Industry 4.0 vision, aiming for fully automated and highly efficient semiconductor manufacturing systems.\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "1. **Data Collection and Integration**: Establish a robust data infrastructure that aggregates customer design files, measurement data, and production outputs from various stages of the RD process for the specified nodes. This involves both structured and unstructured data, such as DRC reports and lithography simulation outputs[8][6].\n",
    "2. **Database Architecture**: Develop a scalable database architecture capable of handling vast amounts of data generated throughout the RD process. This system should support efficient querying and retrieval of information essential for both design and manufacturing optimization[10].\n",
    "3. **AI Model Development**: Utilize large language models (LLMs) to facilitate communication among design, simulation, and manufacturing teams. This step involves training LLMs on relevant technical documents to enhance their understanding and response capabilities in semiconductor design and manufacturing contexts[1][11].\n",
    "4. **Predictive Analytics Implementation**: Deploy AI algorithms focused on predictive analytics to analyze historical data from the RD process. This will assist in identifying patterns and predicting potential outcomes, allowing engineers to make informed decisions early in the design cycle[7].\n",
    "5. **Optimization and Control Systems**: Implement APC systems powered by AI to monitor manufacturing processes in real-time. These systems will enable dynamic adjustments based on sensor data, thereby improving yield rates and operational efficiency across the various technical nodes[5].\n",
    "6. **Feedback Loop and Continuous Improvement**: Establish a feedback mechanism to continually assess and refine the AI models and control systems based on performance metrics and production outcomes. This will ensure that the RD AI project remains aligned with the latest industry advancements and operational needs[11][7].\n",
    "By following these steps, the project aims to create a comprehensive AI-driven data system that supports the efficient and effective development of semiconductor technologies within the critical range of 28 nm to 200 nm nodes.\n",
    "\n",
    "# RD AI Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "The RD AI Project aims to develop a comprehensive AI-driven database that encompasses all relevant data from research and development (R&D) processes within the semiconductor foundry sector, specifically focusing on technology nodes ranging from 28 nm to 200 nm. The project intends to integrate various data types, including client design files, simulation records, measurement data, and output documentation, to establish a robust framework for AI training datasets.\n",
    "\n",
    "## Goals\n",
    "\n",
    "The primary objectives of the RD AI Project are to create a training dataset for AI models that will facilitate enhanced design and manufacturing processes. This initiative is expected to enable more effective decision-making through predictive analytics and optimization techniques applied within semiconductor fabrication and design[12][13].\n",
    "\n",
    "## Implementation Pathway\n",
    "\n",
    "The implementation of the RD AI Project will follow a structured two-phase approach:\n",
    "\n",
    "# Challenges and Considerations\n",
    "\n",
    "The implementation of AI-based data systems in the semiconductor wafer fabrication industry, particularly for research and development (RD) databases, presents numerous challenges and considerations. Understanding these challenges is critical for successfully integrating AI technologies into semiconductor processes.\n",
    "\n",
    "## Data and System Complexities\n",
    "\n",
    "One of the foremost challenges is managing the complexities inherent in data and system integration. The semiconductor industry generates vast amounts of data, particularly from various technology nodes ranging from 28 nm to 200 nm. This data often resides in disparate legacy systems, which can lead to inefficiencies and difficulties in data retrieval and management[14][11]. Legacy data technology modernization is essential; however, it requires a strategic approach to navigate the complexities of obsolete systems while ensuring data quality for real-time analytics[15][11].\n",
    "\n",
    "## Talent Shortage and Skill Gaps\n",
    "\n",
    "The semiconductor industry is currently facing a significant talent shortage, exacerbated by an aging workforce and the rapid evolution of AI technologies[16]. Companies need skilled professionals such as data scientists, machine learning engineers, and infrastructure architects to develop and implement AI models effectively[17][18]. As AI becomes more integral to RD processes—such as DRC (Design Rule Check), layout optimization, and lithography simulation—there is a pressing need for organizations to recruit or train personnel who possess these specialized skills[19][17].\n",
    "\n",
    "## Over-reliance on Black-box Models\n",
    "\n",
    "Another challenge is the over-reliance on black-box AI models, which can hinder interpretability and transparency in decision-making processes. In the semiconductor manufacturing domain, where precision and reliability are paramount, stakeholders must ensure that AI systems provide actionable insights and are interpretable by engineers and management alike[14][20].\n",
    "\n",
    "## Security and Data Privacy\n",
    "\n",
    "Security and data privacy also present significant hurdles, particularly when dealing with sensitive customer information within AI systems. Many organizations require on-premises deployments to safeguard data, which complicates infrastructure management and scaling efforts[21]. Establishing robust security protocols is essential to mitigate risks while maintaining compliance with regulations regarding data ownership and privacy[22].\n",
    "\n",
    "## Strategic Implementation Paths\n",
    "\n",
    "To navigate these challenges effectively, semiconductor companies should adopt a phased approach to AI development. The initial focus could be on predictive analytics to forecast RD outputs and optimize processes, followed by efforts to enhance production efficiency through AI-driven optimization techniques[16]. A well-defined strategy for data sources and types is critical, including establishing a comprehensive data pipeline that integrates various datasets from RD, such as customer design archives and measurement data, into a unified AI framework[23][24].\n",
    "\n",
    "# Steps to Build an AI-Based Data System\n",
    "\n",
    "## 1. Define Objectives and Scope\n",
    "\n",
    "### 1.1 Identify Key Data Sources\n",
    "\n",
    "To establish a robust AI-based data system for semiconductor manufacturing, the first step is to clearly define the objectives and the scope of the project. The focus should be on collecting and managing data relevant to the R&D process from 28nm to 200nm technology nodes.\n",
    "- Customer design files\n",
    "- R&D simulation files\n",
    "- Measurement data from R&D outputs\n",
    "- Relevant documentation associated with R&D processes\n",
    "\n",
    "### 1.2 Establish Data Infrastructure\n",
    "\n",
    "A solid data infrastructure is crucial.\n",
    "- High-velocity data storage solutions capable of handling large datasets generated from design and manufacturing processes[25].\n",
    "- A semantic model to unify data from diverse domains, ensuring that different datasets can be integrated and analyzed effectively[25].\n",
    "\n",
    "## 2. Data Collection and Preprocessing\n",
    "\n",
    "### 2.1 Data Format and Types\n",
    "\n",
    "Data will be collected in various formats, including:\n",
    "- CAD files for design documentation (e.g.\n",
    "- Simulation outputs in formats such as CSV or JSON\n",
    "- Measurement data from sensors in real-time formats like MQTT or REST API.\n",
    "\n",
    "### 2.2 ETL Pipeline Development\n",
    "\n",
    "The ETL (Extract, Transform, Load) pipeline should be developed to ensure efficient data handling:\n",
    "- **Extract**: Use automated scripts to pull data from multiple sources (CAD tools, measurement devices).\n",
    "- **Transform**: Clean and preprocess the data to handle missing values, outliers, and standardize formats[26].\n",
    "- **Load**: Store the processed data in a centralized database (e.g., SQL or NoSQL databases) that supports fast querying and retrieval.\n",
    "\n",
    "### 2.3 Data Cleansing Requirements\n",
    "\n",
    "Data cleansing is vital to ensure the quality of the data:\n",
    "- Remove duplicates and irrelevant data entries.\n",
    "- Validate data integrity against known standards to ensure accuracy, especially in measurement data[26].\n",
    "\n",
    "## 3. Implement AI Models\n",
    "\n",
    "### 3.1 Model Selection\n",
    "\n",
    "Choosing the right AI model is essential for the project's success.\n",
    "- DRC (Design Rule Check) algorithms to validate design adherence to specified rules.\n",
    "- Lithography simulation models to predict the outcomes of manufacturing processes.\n",
    "\n",
    "### 3.2 Training Data Preparation\n",
    "\n",
    "Prepare training datasets for AI models:\n",
    "- Gather and label datasets from historical R&D outputs, including successful and unsuccessful designs.\n",
    "- Use domain-specific parameters (e.g., transistor count, node specifics) as features to inform the AI models about design complexity[27].\n",
    "\n",
    "## 4. Integrate AI and Data Systems\n",
    "\n",
    "### 4.1 Framework for AI Integration\n",
    "\n",
    "Develop a framework to integrate AI capabilities with the existing data systems.\n",
    "- Utilizing Large Language Models (LLMs) to enhance data interaction through conversational interfaces, allowing engineers to query data easily and gain insights[4][28].\n",
    "- Setting up APIs to connect different software systems, enabling seamless data flow and real-time analysis.\n",
    "\n",
    "### 4.2 Decision Support Systems\n",
    "\n",
    "Implement decision support systems that leverage AI to assist engineers in making informed decisions based on real-time data analysis and predictions[29][27].\n",
    "\n",
    "## 5. Continuous Improvement and Iteration\n",
    "\n",
    "### 5.1 Monitor Performance\n",
    "\n",
    "Establish a feedback loop to continuously monitor the performance of AI models and the data system. Use observability tools to track data integrity and model performance over time[30].\n",
    "\n",
    "### 5.2 Iterative Development\n",
    "\n",
    "Implement an iterative approach to enhance models based on new data and insights. As data accumulates, refine AI models to improve prediction accuracy and operational efficiency.\n",
    "\n",
    "## 6. Future Directions\n",
    "\n",
    "### 6.1 Scalability\n",
    "\n",
    "Plan for future scalability by designing systems that can adapt to newer technology nodes (beyond 200nm) and integrate more complex datasets from manufacturing processes[26].\n",
    "\n",
    "### 6.2 Focus on Optimization\n",
    "\n",
    "Transition from predictive capabilities to optimization strategies that can enhance production efficiency, utilizing AI to suggest design improvements and process adjustments based on historical data and real-time analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言\n",
    "\n",
    "随着半导体行业的迅猛发展，尤其是在28 nm到20 nm技术节点的研发中，数据驱动的方法变得尤为重要。为了提升研发效率、降低成本和提高产品质量，建立一个AI驱动的数据库系统已成为迫在眉睫的任务[1][2]。本项目旨在整合晶圆代工厂的研发（R&D）数据库，包括客户设计档案、R&D模拟档案、设备与量测数据等内容，以实现更高效的数据管理与分析。\n",
    "在此背景下，利用大型语言模型（LLM）技术可以显著提升数据分析能力。LLM的快速进展为制造业提供了全新的机遇，能够优化流程、提高效率，并推动创新[3]。通过AI的辅助，研发团队可以有效地从海量数据中提取有价值的信息，从而促进决策制定和设计优化。\n",
    "本项目将采取两步走的发展策略。第一步，明年的目标集中在通过AI进行预测，这将包括设计端的DRC（设计规则检查）、布局优化和光刻模拟系统的整合，建立思维链、验证链及价值链。第二步，后年将着重于通过AI优化生产流程，包括APC（先进过程控制）、FDC（故障检测与分类）、YPA（良率预测分析）等系统的集成[4]。\n",
    "实现这一目标的步骤包括明确数据源的获取与处理流程，建立AI模型进行数据分析和预测，优化与整合系统以提升整体效率。通过这一系列措施，我们期望在R&D部门实现全面的AI辅助数据分析能力，确保在激烈的市场竞争中保持领先地位。\n",
    "\n",
    "# 项目范畴\n",
    "\n",
    "## 数据库范围\n",
    "\n",
    "该项目旨在建立一个AI驱动的半导体晶圆代工厂的研发（R&D）数据库，覆盖从28nm到20nm技术节点的所有相关数据。数据库将包含以下关键内容：\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "\n",
    "## 长远目标\n",
    "\n",
    "### AI辅助的数据分析能力\n",
    "\n",
    "在半导体晶圆代工行业，长远目标是实现AI辅助的数据分析能力，以提高研发部门的效率，降低成本，并提升产品质量。这一目标涉及建立一个全面的研发（R&D）数据库系统，集中管理与28nm到20nm技术节点相关的所有数据，涵盖客户设计档案、研发模拟档案、设备与量测数据等[1][5]。\n",
    "\n",
    "#### 数据源与流程\n",
    "\n",
    "建立数据库系统的第一步是确定数据源。数据源包括但不限于客户设计档案、研发设备的操作数据、量测数据和输出文件等。数据流程将涉及以下几个关键步骤：\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "\n",
    "#### AI技术的应用\n",
    "\n",
    "在数据分析过程中，将采用AI技术来实现预测和优化的目标。未来的AI发展分为两个阶段：\n",
    "- **第一阶段**（明年）：集中在通过AI进行预测。例如，利用机器学习模型预测工艺参数的变化对产品性能的影响，以提高决策的准确性[6][5]。\n",
    "- **第二阶段**（后年）：转向通过AI优化生产过程。通过使用生成对抗网络（GAN）或其他高级模型，优化设计规则检查（DRC）、布局优化和光刻模拟等子系统，建立更为高效的思维链、验证链和价值链[1][3][7]。\n",
    "\n",
    "#### 系统整合\n",
    "\n",
    "系统的整合将以STCO（智能制造和生产优化）为核心，确保各个子系统的无缝连接。这种整合将增强数据的流动性，使各个环节的数据都能够被实时分析和利用。利用大型语言模型（LLM）技术，可以实现自然语言处理，使研发人员能够通过自然语言查询系统，获取即时的分析结果和建议[3][2]。\n",
    "通过以上步骤的实施，可以在R&D部门建立一个高效的AI驱动数据库系统，全面提升半导体晶圆代工的研发能力和市场竞争力。\n",
    "\n",
    "## 实施步骤\n",
    "\n",
    "### 1. 数据源整合\n",
    "\n",
    "首先，需要整合来自不同来源的数据，包括：\n",
    "- 客户设计档案：包括客户提供的电路设计文件和规格说明。\n",
    "- R&D模拟档案：模拟结果及分析文件，以评估设计方案的可行性。\n",
    "- R&D设备数据：设备的性能记录及维护日志，以监控设备状态和有效性。\n",
    "- 测量数据：在制造过程中采集的实时数据，以用于质量控制和过程优化[5][8]。\n",
    "\n",
    "### 2. 数据流及管理\n",
    "\n",
    "数据的流动和管理将遵循以下流程：\n",
    "- **数据采集**：使用物联网(IoT)设备和传感器收集实时数据，并通过数据采集系统将数据传输至中央数据库。\n",
    "- **数据预处理**：实施数据清洗和预处理，包括去除重复数据、填补缺失值和标准化数据格式[1][9]。\n",
    "- **数据存储**：设计一个结构化的数据库，以便高效存储、查询和检索R&D数据，采用大数据平台服务以支持高并发的读写操作[1][10]。\n",
    "\n",
    "### 3. AI辅助数据分析\n",
    "\n",
    "为实现AI辅助的数据分析能力，将通过以下步骤来利用AI技术：\n",
    "- **建模**：利用机器学习(ML)算法对历史数据进行训练，以预测未来的研发成果和生产效率。例如，通过分析历史量测数据，建立 yield prediction 和 analysis 模型，以预测未来的良率[11][8]。\n",
    "- **优化算法**：基于AI的优化技术，将应用于设计规则检查（DRC）、布局优化、光刻模拟等关键领域，以提升设计过程的效率与准确性[12][13]。\n",
    "- **决策支持**：通过大语言模型（LLM）技术，开发自然语言处理（NLP）系统，帮助研发人员进行信息检索、文档生成及智能决策支持，降低对数据分析专家的依赖[3][14][10]。\n",
    "\n",
    "### 4. 系统集成\n",
    "\n",
    "整合设计端与制程端的多个子系统，以实现端到端的研发流程优化：\n",
    "- **设计端**：建立DRC、布局优化及光刻模拟三子系统的联动机制，确保设计逻辑的连贯性和可验证性。\n",
    "- **制程端**：实施先进过程控制（APC）、故障检测与分类（FDC）、良率分析（YPA）、工艺变动分析（WRA）及过程验证分析（PVA）等模块，以实现生产过程的动态监控与优化[13][15][9]。\n",
    "\n",
    "### 5. 长期目标与展望\n",
    "\n",
    "项目的长远目标是通过AI实现全面的生产优化，重点在未来一年集中于预测能力的提升，后年则致力于生产过程的全面优化。系统的最终整合将以智能生产控制（STCO）为核心，以形成有效的思维链、验证链和价值链，助力半导体行业的数字化转型[12][16][11]。\n",
    "\n",
    "# RD AI 项目\n",
    "\n",
    "## 项目目标\n",
    "\n",
    "本项目旨在建立一个AI驱动的研发（R&D）数据库系统，重点集中于28 nm到20 nm技术节点的半导体晶圆代工厂的研发数据管理。系统将涵盖客户设计档案、R&D模拟档案、设备及量测数据等，以提高研发部门的数据分析能力，最终实现提升效率、降低成本及提高质量的目标[12][16]。\n",
    "\n",
    "# AI发展步骤\n",
    "\n",
    "## 数据库系统的规划与设计\n",
    "\n",
    "为了建立一个AI驱动的研发数据库系统，首先需要明确数据库的范畴和目标。系统将集中于28nm到20nm技术节点的所有研发数据，包括客户设计档案、R&D模拟档案、设备及量测数据等。这些数据将构成基础，为后续的AI辅助数据分析提供支持[6][17]。\n",
    "\n",
    "### 数据源的确定\n",
    "\n",
    "1.\n",
    "- 客户设计档案（设计规则检查DRC、布局优化、光刻模拟等）。\n",
    "- R&D模拟档案和设备数据。\n",
    "- 量测数据和输出文件。\n",
    "2.\n",
    "\n",
    "## AI辅助数据分析的实施步骤\n",
    "\n",
    "### 第一阶段：预测分析\n",
    "\n",
    "1.\n",
    "- 采用大语言模型（LLM）进行初步的预测分析，如识别潜在的设计问题、材料特性等[7][3][2]。\n",
    "- 收集并整理历史数据作为训练集，通过模型进行学习。\n",
    "2.\n",
    "- 将研发数据输入模型，进行初步的预测。\n",
    "- 收集模型反馈，针对性调整模型参数以提高预测准确性。\n",
    "\n",
    "### 第二阶段：优化生产\n",
    "\n",
    "1.\n",
    "- 基于第一阶段的预测结果，开发新的模型以优化生产流程，包括先进制程控制（APC）、故障诊断（FDC）、样本分析（YPA）等功能。\n",
    "- 整合模型，形成全面的生产优化系统。\n",
    "2.\n",
    "- 对优化后的生产系统进行验证，确保其有效性和准确性。\n",
    "- 收集使用数据，进行模型的迭代更新，以不断提高系统的性能和预测能力。\n",
    "\n",
    "## 综合实施计划\n",
    "\n",
    "1.\n",
    "- 建立思维链、验证链和价值链，确保不同系统之间的高效连接与协同工作。\n",
    "2.\n",
    "- 明年的目标集中在通过AI进行预测，后年则转向通过AI优化生产，确保系统的渐进式实施和效果评估。\n",
    "3.\n",
    "- 组织团队内部培训，确保所有员工对AI系统有充分的理解与应用能力，以促进研发与生产的高效协作。\n",
    "通过以上步骤，可以逐步实现AI驱动的研发数据库系统，并提升半导体行业的研发效率和生产质量。\n",
    "\n",
    "# 数据库系统建设步骤\n",
    "\n",
    "## 项目规划与需求分析\n",
    "\n",
    "在建立AI驱动的R&D数据库系统的初期，首先需要进行全面的项目规划与需求分析。此阶段应明确数据库的目标范围，包括涵盖28 nm到20 nm技术节点的所有相关数据，如客户设计档案、R&D模拟档案、设备和测量数据等[18]。通过与相关部门的沟通，确定每项数据类型的重要性及其在研发流程中的应用场景，以制定优先级和实施路线图。\n",
    "\n",
    "## 数据源整合\n",
    "\n",
    "### 数据收集\n",
    "\n",
    "整合数据源是构建数据库系统的关键步骤。需识别所有潜在的数据源，包括内部R&D文档、客户设计文件、生产设备的实时数据等[19]。考虑到数据的多样性，可以从以下几类获取数据：\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "\n",
    "### 数据存储与管理\n",
    "\n",
    "选择合适的数据库技术（如关系型数据库或NoSQL）以支持大数据存储和高效查询。此步骤需关注数据结构的设计，确保数据的可扩展性和高效性[16]。在构建数据管理框架时，考虑到数据的清洗、整合和实时更新，以保持数据的准确性和时效性。\n",
    "\n",
    "## AI技术应用\n",
    "\n",
    "### 数据分析能力建设\n",
    "\n",
    "建立AI驱动的数据分析能力是提高研发效率的核心。使用大型语言模型（LLM）技术，可以帮助团队从大量的研发数据中提取有价值的信息。具体步骤包括：\n",
    "1.\n",
    "2.\n",
    "\n",
    "### 预测与优化\n",
    "\n",
    "初步阶段集中在利用AI进行预测，以帮助识别潜在的研发问题和市场机会。随着项目的推进，后续将转向通过AI优化生产流程。具体步骤如下：\n",
    "1.\n",
    "2.\n",
    "\n",
    "## 系统整合与验证\n",
    "\n",
    "在系统的最终集成阶段，需将各子系统（如DRC、布局优化、光刻仿真等）串联起来，形成完整的工作流。这一过程中，确保思维链、验证链和价值链的建立至关重要，以保证各环节的高效协作[16]。通过实施持续的验证和反馈机制，确保系统不断优化，适应未来技术变化的需求。\n",
    "\n",
    "## 长期目标与持续改进\n",
    "\n",
    "项目的长期目标是不断完善AI辅助的数据分析能力，以应对日益复杂的R&D挑战。持续收集和分析数据反馈，以支持数据库的演进和AI模型的改进，最终实现研发过程的高效化、低成本和高质量[18]。\n",
    "\n",
    "# AI辅助数据分析\n",
    "\n",
    "## 预测分析\n",
    "\n",
    "在初期阶段，重点通过AI进行预测分析，具体步骤包括：\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "\n",
    "## 优化生产\n",
    "\n",
    "在后续阶段，系统将集中于通过AI来优化生产过程：\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\n",
    "# 实施计划\n",
    "\n",
    "## 项目概述\n",
    "\n",
    "本项目旨在建立一个AI驱动的数据库系统，专注于半导体晶圆代工厂的研发（R&D）数据，以提高数据分析能力，从而实现效率提升、成本降低和质量改善。该系统将聚焦于28 nm到20 nm技术节点，包含客户设计档案、研发模拟档案、设备和量测数据等内容。\n",
    "\n",
    "## 数据库架构设计\n",
    "\n",
    "### 数据源\n",
    "\n",
    "项目将整合多种数据源，包括：\n",
    "- **客户设计档案**：存储客户提供的设计文件和技术规格。\n",
    "- **研发模拟档案**：记录在不同技术节点下进行的模拟实验数据。\n",
    "- **研发设备数据**：包含设备的运行状态、产量和故障记录等。\n",
    "- **量测数据**：来自生产线的量测数据，用于质量控制和工艺优化。\n",
    "- **R&D输出文件**：最终产品的数据报告和分析结果。\n",
    "\n",
    "### 数据流程\n",
    "\n",
    "数据的具体流程将分为以下几个步骤：\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "\n",
    "## 系统整合与价值链建立\n",
    "\n",
    "### 整合方案\n",
    "\n",
    "全系统整合将以智能制造总控（STCO）为核心，通过以下方式实现：\n",
    "1.\n",
    "2.\n",
    "\n",
    "### 价值链构建\n",
    "\n",
    "通过整合R&D和生产数据，构建完整的价值链，提升决策效率。具体措施包括：\n",
    "- **信息透明**：通过数据可视化技术，提高各个环节的信息透明度，帮助团队快速响应市场变化。\n",
    "- **协同创新**：基于AI分析结果，促进跨部门合作，推动技术创新和工艺改进。\n",
    "\n",
    "## 结论\n",
    "\n",
    "在半导体晶圆代工行业中，建立一个AI驱动的研发数据库系统是实现高效、低成本和高质量生产的关键步骤。通过整合28 nm到20 nm技术节点的所有研发数据，包括客户设计档案、模拟档案、设备和量测数据，可以为公司提供强大的数据分析能力，从而提升决策效率和产品质量[1][20]。\n",
    "未来的AI发展分为两个阶段。第一阶段将集中在利用AI进行预测，主要是通过大语言模型（LLM）技术优化数据分析过程，提取有价值的信息，以帮助设计和制造环节。例如，通过LLM进行DRC、布局优化和光刻模拟等，可以有效串联相关子系统，形成思维链、验证链和价值链[3][2]。第二阶段则致力于通过AI实现生产过程的优化，将预测性分析转化为实际操作的改进，使用APC（先进过程控制）、FDC（故障检测与分类）、YPA（Yield Process Analysis）、WRA（Wafer Reliability Assessment）和PVA（Process Variation Analysis）等工具来提升制造精度和良率[1][21]。\n",
    "在实现这一目标的过程中，数据源的选择至关重要。应当构建一个涵盖各类数据的统一平台，整合来自不同环节的数据流，以支持全面的数据分析和决策支持系统。此外，采用高效的数据处理和存储技术，确保数据的实时性和可获取性也是成功的关键。通过引入先进的数据分析技术和工具，结合AI的强大能力，可以实现对研发数据的深入分析与挖掘，从而推动半导体行业的持续创新与发展。\n",
    "\n",
    "# 结论\n",
    "\n",
    "在半导体晶圆代工行业中，建立一个AI驱动的研发数据库系统是实现高效、低成本和高质量生产的关键步骤。通过整合28 nm到20 nm技术节点的所有研发数据，包括客户设计档案、模拟档案、设备和量测数据，可以为公司提供强大的数据分析能力，从而提升决策效率和产品质量[1][20]。\n",
    "未来的AI发展分为两个阶段。第一阶段将集中在利用AI进行预测，主要是通过大语言模型（LLM）技术优化数据分析过程，提取有价值的信息，以帮助设计和制造环节。例如，通过LLM进行DRC、布局优化和光刻模拟等，可以有效串联相关子系统，形成思维链、验证链和价值链[3][2]。第二阶段则致力于通过AI实现生产过程的优化，将预测性分析转化为实际操作的改进，使用APC（先进过程控制）、FDC（故障检测与分类）、YPA（Yield Process Analysis）、WRA（Wafer Reliability Assessment）和PVA（Process Variation Analysis）等工具来提升制造精度和良率[1][21]。\n",
    "在实现这一目标的过程中，数据源的选择至关重要。应当构建一个涵盖各类数据的统一平台，整合来自不同环节的数据流，以支持全面的数据分析和决策支持系统。此外，采用高效的数据处理和存储技术，确保数据的实时性和可获取性也是成功的关键。通过引入先进的数据分析技术和工具，结合AI的强大能力，可以实现对研发数据的深入分析与挖掘，从而推动半导体行业的持续创新与发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n",
    "\n",
    "The AI-driven database system project in the semiconductor wafer fabrication sector represents a cutting-edge initiative aimed at revolutionizing research and development (R&D) processes through advanced data analysis and artificial intelligence (AI). This project focuses specifically on technology nodes ranging from 28 nm to 20 nm, integral to the R&D activities of wafer foundries. By integrating AI, the project aspires to enhance efficiency, reduce costs, and improve quality within the semiconductor industry, which is known for its fast-paced advancements and complex production challenges. The system will manage a comprehensive array of R&D data, including customer design files, simulation archives, and measurement data, positioning itself as a transformative tool in the high-stakes semiconductor field.[1][2]\n",
    "Notably, this endeavor plans to deploy AI in two major phases: prediction and optimization. The initial phase, set for the upcoming year, emphasizes leveraging AI to enhance predictive capabilities, which is crucial for proactive decision-making and risk mitigation in semiconductor R&D. The subsequent phase will shift focus to optimizing production processes, aiming to improve yield and efficiency through AI-driven solutions such as Advanced Process Control (APC) and Fault Detection and Classification (FDC). This strategic two-step approach aligns with the long-term goal of integrating System and Technology Co-Optimization (STCO) frameworks, thereby ensuring a seamless transition from research to production optimization.[3]\n",
    "A significant aspect of the project is the utilization of large language models (LLMs) for AI-assisted data analysis. These models are anticipated to automate various R&D tasks, from validating design specifications to executing analysis tasks, all through natural language processing capabilities. The deployment of LLMs aims to democratize access to sophisticated data analytics, enabling a broader range of employees to engage in high-level data interpretation without necessitating extensive expertise. This innovative use of LLMs is set to enhance the overall usability and efficiency of the database system, facilitating a more inclusive approach to data-driven decision-making within semiconductor R&D environments.[4][5]\n",
    "However, integrating such advanced AI systems into existing semiconductor R&D processes presents several challenges, including compatibility with legacy systems and ensuring data quality and reliability. The project addresses these challenges by adopting a modular integration strategy, which minimizes complexity and allows for gradual, risk-mitigated implementation. This approach ensures that the AI-driven database system is not only a short-term enhancement but also a scalable platform that can adapt to future advancements in AI and electronic design automation (EDA). By systematically overcoming these hurdles, the project aims to establish a robust foundation for ongoing innovation and competitiveness in the semiconductor industry.[6][7]\n",
    "---\n",
    "**Sources:**\n",
    "1. Source for the project's scope and goals.\n",
    "2. Source for industry relevance and technological nodes.\n",
    "3. Source for AI deployment phases and methodologies.\n",
    "4. Source for the use of LLMs and AI-assisted data analysis.\n",
    "5. Source for democratization of data analytics.\n",
    "6. Source for challenges related to legacy systems.\n",
    "7. Source for scalability and strategic planning.\n",
    "\n",
    "# Project Overview\n",
    "\n",
    "The project aims to establish an AI-driven database system for a semiconductor wafer fabrication plant's R&D department, specifically targeting the 28 nm to 20 nm technology nodes. This initiative will encompass a comprehensive range of R&D data, including customer design files, simulation archives, equipment, and measurement data, as well as R&D outputs and documents. The long-term objective is to implement AI-assisted data analysis capabilities to enhance efficiency, reduce costs, and improve quality in the R&D sector.\n",
    "\n",
    "## AI Empowerment Directions\n",
    "\n",
    "### Design Phase\n",
    "\n",
    "In the design phase, the focus will be on enhancing processes such as Design Rule Checking (DRC), layout optimization, and lithography simulation. A system will be developed to integrate these subsystems into a cohesive thought chain, verification chain, and value chain[1].\n",
    "\n",
    "### Process Phase\n",
    "\n",
    "On the process side, the project will cover Advanced Process Control (APC), Fault Detection and Classification (FDC), Yield Prediction and Analysis (YPA), Wafer Run Analysis (WRA), and Process Variability Analysis (PVA)[2].\n",
    "\n",
    "## AI Integration Phases\n",
    "\n",
    "The AI introduction is planned in two phases. The first phase, scheduled for the next year, will emphasize predictive capabilities through AI, while the following year will concentrate on production optimization. The complete system integration will be primarily driven by System and Technology Co-Optimization (STCO)[3].\n",
    "\n",
    "## Implementation Plan\n",
    "\n",
    "The implementation of an AI-driven database system within the semiconductor wafer foundry's R&D department focuses on leveraging AI for enhanced data analysis capabilities, specifically targeting the 28 nm to 20 nm technology nodes. This plan outlines the strategic steps necessary to establish this database system and details the utilization of large language models (LLMs) in conjunction with data analytics.\n",
    "\n",
    "# Database System Development\n",
    "\n",
    "The development of an AI-driven database system in the semiconductor industry, specifically focusing on RD (Research and Development) activities, involves several crucial steps to ensure efficient data management and AI integration. This database system is designed to encompass data related to technology nodes ranging from 28 nm to 20 nm, including customer design files, R&D simulation archives, measurement data, and R&D output files.\n",
    "\n",
    "## Step 1: Data Collection and Integration\n",
    "\n",
    "The first step in building this database system is the collection and integration of data from various sources. Semiconductor manufacturing generates a large volume of data during processes such as wafer fabrication and testing[4]. Data sources include GDSII files, foundry process control monitor data, and test executive software outputs[4]. Additionally, data from manufacturing processes such as oxidation, photo lithography, and etching must be integrated[5]. Ensuring that data from disparate sources and formats is successfully integrated is one of the primary challenges faced by semiconductor manufacturers[6].\n",
    "\n",
    "## Step 2: Establishing Data Management Practices\n",
    "\n",
    "Once the data is collected, establishing robust data management practices is paramount to ensuring data quality and reliability. Data management is crucial for creating a foundation of trust and efficacy in AI-driven processes[7]. Companies should adopt proactive data management strategies, ensuring that the data feeding into the database system is accurate and up-to-date. This includes connecting different data sources to enable both feed-forward and feedback processes in manufacturing[8].\n",
    "\n",
    "## Step 3: Leveraging Advanced Analytics\n",
    "\n",
    "Advanced analytics play a significant role in transforming raw data into actionable insights. By employing advanced-analytics tools, semiconductor companies can gain valuable predictive insights, guiding decision-making and process optimization[9]. Predictive models can be developed using chip- and block-level parameters as proxies for design complexity, allowing for a more efficient R&D process[10].\n",
    "\n",
    "## Step 4: AI-Assisted Data Analysis\n",
    "\n",
    "Integrating AI into the R&D database system can enhance data analysis capabilities, ultimately improving efficiency, reducing costs, and enhancing product quality. AI technologies, including large language models (LLMs), can automate workflows, reduce human errors, and validate design specifications[11]. LLMs can also provide natural language explanations and prompt analysis tasks, improving the accessibility and usability of the database system[11].\n",
    "\n",
    "## Step 5: AI-Powered Process Optimization\n",
    "\n",
    "In the subsequent stages, the focus will shift towards process optimization through AI. The aim is to leverage AI for predictive capabilities initially, and later for optimizing production processes. This involves integrating systems like APC (Advanced Process Control), FDC (Fault Detection and Classification), and other related systems to enhance process yields and efficiency[12]. The full system integration will prioritize STCO (Supply Chain Optimization), facilitating a streamlined manufacturing process.\n",
    "By systematically implementing these steps, semiconductor companies can build a comprehensive AI-driven database system that significantly enhances their R&D and manufacturing capabilities.\n",
    "\n",
    "# AI Enablement Strategies\n",
    "\n",
    "The integration of AI into semiconductor R&D processes is set to revolutionize the industry by accelerating innovation and improving efficiency. The primary focus for AI enablement in a semiconductor foundry's R&D database system encompasses both design and process stages. In the design phase, AI can be applied to areas such as Design Rule Check (DRC), layout optimization, and lithography simulation. This involves establishing interconnected systems to form a comprehensive thought chain, validation chain, and value chain[13][14].\n",
    "\n",
    "## Design Phase Enablement\n",
    "\n",
    "### Design Rule Check (DRC) and Layout Optimization\n",
    "\n",
    "AI-driven DRC can enhance the accuracy and speed of design validation by automating error detection and correction processes, thereby reducing human-induced errors in chip design[11]. Layout optimization can further benefit from AI through advanced pattern recognition and predictive analytics, allowing for more efficient space utilization and improved circuit performance[15]. These AI applications democratize decision-making, enabling more employees to utilize sophisticated models without requiring deep data analytics expertise[14].\n",
    "\n",
    "### Lithography Simulation\n",
    "\n",
    "Lithography simulation stands to gain from AI through improved computational models that enhance resolution and mask optimization processes[16][17]. AI models inspired by physics have shown promise in enabling scalable computational lithography, thus overcoming the traditional limitations posed by time-consuming simulation processes[18].\n",
    "\n",
    "## Process Phase Enablement\n",
    "\n",
    "### Advanced Process Control (APC) and Fault Detection and Classification (FDC)\n",
    "\n",
    "In the process phase, APC systems can be augmented with AI to provide real-time feedback and control of manufacturing processes, significantly improving precision and reducing variability[19][20]. AI can also enhance FDC by accurately predicting potential defects, allowing for real-time adjustments that improve yield rates and production efficiency[19][21].\n",
    "\n",
    "### Yield Prediction and Analysis (YPA)\n",
    "\n",
    "AI applications in YPA can analyze vast amounts of production data to identify patterns and optimize process parameters, thereby maximizing semiconductor yield[22]. This predictive approach can lead to a substantial reduction in yield loss, translating to cost savings and increased production capacity[23].\n",
    "\n",
    "### Wafer-to-Wafer and Run-to-Run Adjustments\n",
    "\n",
    "AI-driven analytics tools enable the identification of process bottlenecks and facilitate real-time adjustments at both wafer-to-wafer and run-to-run levels. This optimization ensures better throughput and flexibility in manufacturing processes[24].\n",
    "\n",
    "## Implementation Plan\n",
    "\n",
    "To establish an AI-driven database system, the implementation process will commence with the integration of R&D data sources, including customer design files, R&D simulation records, equipment and measurement data, and output documents. This will be followed by the deployment of AI models tailored to the specific requirements of each R&D phase. The use of Large Language Models (LLMs) will be critical in enhancing data analysis capabilities, enabling teams to interrogate designs, validate specifications, and execute analysis tasks with natural language processing capabilities[11][25].\n",
    "The phased AI enablement strategy will initially focus on predictive capabilities, with subsequent efforts directed toward process optimization. The ultimate goal is to achieve full system integration based on a System-on-Chip (STCO) framework, ensuring seamless data flow and analysis across all R&D processes[23].\n",
    "\n",
    "# Challenges and Solutions\n",
    "\n",
    "The integration of AI-driven database systems within the semiconductor R&D environment presents several challenges and solutions. One of the primary challenges lies in the integration of AI solutions with existing legacy systems. These systems often exhibit compatibility issues, including disparities in data formats and standards, and incompatibility with modern APIs and protocols[26]. Addressing these challenges requires a comprehensive approach to ensure seamless integration.\n",
    "\n",
    "## Legacy System Integration\n",
    "\n",
    "To overcome the integration challenges posed by legacy systems, organizations can adopt a modular integration strategy. This approach allows for the effective incorporation of LLM APIs into existing systems, thereby minimizing complexity and reducing the risk of errors[27]. By implementing modular integration, businesses can ensure easier maintenance and updates, thus laying a solid foundation for unlocking the full potential of LLMs and driving business value through AI-driven language processing.\n",
    "\n",
    "## Data Quality and Compatibility\n",
    "\n",
    "Ensuring data quality and compatibility is another significant challenge. A well-structured data journey is essential for maintaining the quality and reliability of data fed into LLMs, which is crucial for establishing trust and efficacy in AI-driven processes[7]. Organizations must focus on strategic planning and resource allocation to manage data effectively. This includes building a robust data infrastructure and fostering a culture that embraces AI[28].\n",
    "\n",
    "## Workflow Automation and Optimization\n",
    "\n",
    "In the context of semiconductor design and manufacturing, automation plays a pivotal role. AI technologies, such as LLMs, can significantly enhance the automation of workflows by reducing human-induced errors in design specifications and project documents[11]. For instance, once the first draft of a chip's hardware description language (HDL) is created, engineers can utilize LLM chatbots to interrogate the design, validate it against specifications, and rectify any issues through natural language processing[11]. This not only optimizes the design process but also ensures higher precision and efficiency.\n",
    "\n",
    "## Predictive and Optimization Capabilities\n",
    "\n",
    "The phased introduction of AI, initially focusing on predictive capabilities and subsequently on production optimization, provides a strategic roadmap for integration. By leveraging AI to predict and optimize manufacturing processes, organizations can significantly improve efficiency, reduce costs, and enhance product quality[13]. This approach aligns with the long-term goals of integrating AI into R&D processes to boost efficiency and performance[13].\n",
    "\n",
    "# Future Directions\n",
    "\n",
    "The future directions for the AI-driven database system in the semiconductor foundry's R&D department are designed to integrate advanced AI capabilities, focusing initially on prediction and later on production optimization. As the industry moves towards smaller technology nodes such as 28 nm to 20 nm, leveraging AI for enhanced data analysis becomes crucial for maintaining competitive advantage and efficiency[29][30].\n",
    "\n",
    "## Prediction and Optimization\n",
    "\n",
    "The implementation will follow a two-step approach. In the first year, the emphasis will be on employing AI to enhance predictive capabilities across various processes. This includes the design stages such as Design Rule Checking (DRC), layout optimization, and lithography simulation[31][30]. By establishing interconnected systems that form a \"thought chain,\" \"verification chain,\" and \"value chain,\" the system aims to streamline operations and improve design accuracy[11].\n",
    "In the following year, the focus will shift to production optimization using AI. The integration of AI in the production processes such as Advanced Process Control (APC), Fault Detection and Classification (FDC), Yield Prediction and Analysis (YPA), and Wafer Recipe Analysis (WRA) will be crucial[11][26]. This stage will primarily revolve around the use of Short-Term Circuit Optimization (STCO) to ensure holistic system integration[32].\n",
    "\n",
    "## Data Integration and Management\n",
    "\n",
    "A well-structured data journey is pivotal to this system's success, ensuring the quality and reliability of the data used in AI-driven processes[7]. Data sources will encompass comprehensive R&D databases that include client design files, simulation archives, equipment, and measurement data from the 28 nm to 20 nm nodes. Effective data management practices will be crucial in supporting AI analytics and fostering trust in AI outputs[29][7].\n",
    "\n",
    "## Leveraging Large Language Models (LLMs)\n",
    "\n",
    "LLMs will play a transformative role in this project by automating and enhancing various R&D tasks. From interrogating chip designs to validating design specifications, LLMs will provide engineers with advanced tools for natural language processing and prompt analysis tasks[31][11]. By adopting a modular integration strategy, enterprises can minimize complexity and reduce risks, thus unlocking the full potential of LLMs to drive business value[27].\n",
    "\n",
    "## Scalability and Strategic Planning\n",
    "\n",
    "Strategic planning teams, along with financial analysis tools, will be instrumental in setting the stage for future scalability of the AI-driven database system[29]. The goal is to ensure that the integration of AI not only meets current needs but also lays a foundation for future expansion and enhancements in AI capabilities[12]. This involves continuous monitoring of industry trends and advancements in AI and EDA to adapt the system accordingly[31][12]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
